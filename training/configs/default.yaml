# Digital Goldfish Training Configuration

# Environment
env:
  width: 1600
  height: 1200
  max_steps: 2000
  initial_food: 8
  food_spawn_rate: 0.005

  # Hunger mechanic
  hunger:
    initial: 1.0
    decay_rate: 0.001      # ~16s to starve at 60fps
    eat_restore: 0.3       # 3-4 foods to satiate
    penalty_scale: 0.01

  # Exploration reward
  exploration:
    grid_size: 100         # 16x12 cells
    visit_bonus: 0.05
    distance_bonus: 0.001

# Vectorized environment
num_envs: 64

# Policy network
policy:
  num_rays: 16
  num_lateral: 8
  hidden_dim: 64

# PPO Hyperparameters
ppo:
  learning_rate: 3.0e-4
  gamma: 0.99           # Discount factor
  gae_lambda: 0.95      # GAE lambda
  clip_coef: 0.2        # PPO clip coefficient
  ent_coef: 0.05        # Entropy coefficient (increased for exploration)
  vf_coef: 0.5          # Value function coefficient
  max_grad_norm: 0.5    # Gradient clipping

# Training
training:
  total_timesteps: 1_000_000
  num_steps: 128        # Steps per rollout
  num_minibatches: 4    # Minibatches per update
  update_epochs: 4      # Epochs per update
  anneal_lr: true       # Anneal learning rate

# Logging
logging:
  log_interval: 10      # Log every N updates
  save_interval: 100    # Save checkpoint every N updates

# Paths
paths:
  checkpoint_dir: checkpoints
  log_dir: logs
