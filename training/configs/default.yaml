# Digital Goldfish Training Configuration
#
# Multi-fish environment with shared policy
# Each fish has 60 observation features:
#   - Raycasts: 32 (16 rays x 2)
#   - Lateral line: 16 (8 sensors x 2)
#   - Proprioception: 4
#   - Internal state: 4
#   - Social: 4 (nearest_dist, nearest_angle, num_nearby, heading_diff)
#
# Action space (3D per fish):
#   - speed [0, 1]: Desired forward speed
#   - direction [-1, 1]: Turn rate
#   - urgency [0, 1]: Movement intensity
#
# Device: Auto-selects MPS (Mac) > CUDA > CPU

# Environment
env:
  width: 800
  height: 600
  max_steps: 2000
  initial_food: 8
  food_spawn_rate: 0.02
  num_fish: 3  # Number of fish per environment

  # Exploration reward
  exploration:
    grid_size: 100
    visit_bonus: 0.05
    distance_bonus: 0.001

# Vectorized environment
num_envs: 64

# Policy network
policy:
  num_rays: 16
  num_lateral: 8
  hidden_dim: 64

# PPO Hyperparameters
ppo:
  learning_rate: 3.0e-4
  gamma: 0.99           # Discount factor
  gae_lambda: 0.95      # GAE lambda
  clip_coef: 0.2        # PPO clip coefficient
  ent_coef: 0.05        # Entropy coefficient
  vf_coef: 0.5          # Value function coefficient
  max_grad_norm: 0.5    # Gradient clipping

# Training
training:
  total_timesteps: 10_000_000  # 10M steps
  num_steps: 128        # Steps per rollout
  num_minibatches: 4    # Minibatches per update
  update_epochs: 4      # Epochs per update
  anneal_lr: true       # Anneal learning rate

# Logging
logging:
  log_interval: 10      # Log every N updates
  save_interval: 100    # Save checkpoint every N updates

# Paths
paths:
  checkpoint_dir: checkpoints
  log_dir: logs
